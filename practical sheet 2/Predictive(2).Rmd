---
title: "PREDICTIVE ANALYTICS"
author: "Oindrila Chakraborty"
date: "2026-02-01"
output: word_document
---

## Problem Set 2: Linear Regression.
### 1 Problem to demonstrate that the population regression line is fixed, but least square regression line varies.

**Suppose the population regression line is given by Y = 2 + 3x, while the data comes from the model y = 2 + 3x + Ïµ.**

**Step 1:** For x in the range [5,10] graph the population regression line.

**Step 2:** Generate xi(i = 1, 2, .., n) from Uniform(5, 10) and Ïµi(i = 1, 2, .., n)
from N(0, 16). Hence, compute y1, y2, .., yn.

**Step 3:** On the basis of the data (xi, yi)(i = 1, 2, .., n) generated in Step 2,
report the least squares regression line.

**Step 4:** Repeat steps 2-3 five times. Graph the 5 least squares regression lines
over the population regression line obtained in Step 1.
Interpret the findings.

**Take n = 50. Set the seed as seed=123.**

```{r}
#Step 1.
x=seq(5,10,length.out=200)
x
y=2+3*x
y
plot(x,y,type='l',col="red")
```

```{r}
#Step 2.
set.seed(123)
xi=runif(50,5,10)
xi
ei=rnorm(50,0,4)
ei
yi=2+3*xi+ei
yi
plot(x,y,type='l',col="red")
points(xi, yi, col = "blue", pch = 16)
```

```{r}
#Step 3.
model=lm(yi~xi)
model
summary(model)
plot(xi, yi, col = "blue", pch = 16)
abline(model,col='red',lwd = 2)
coef(model)
```



```{r}
# Step 4.
set.seed(123)
n=50

# Population regression line
x_pop=seq(5, 10, length.out = 100)
y_pop=2 + 3 * x_pop

plot(x_pop, y_pop, type = "l", lwd = 3, col = "black",
     xlab = "x", ylab = "y",
     main = "Population Line and Sample Regression Lines")

beta_table=data.frame(
  Simulation = 1:5,
  Beta_0 = numeric(5),
  Beta_1 = numeric(5)
)

colors=c("red", "blue", "green", "purple", "orange")

for (i in 1:5) {
  x=runif(n, 5, 10)
  eps=rnorm(n, 0, 4)
  y=2 + 3 * x + eps
  
  fit=lm(y ~ x)
  
  abline(fit, col = colors[i], lwd = 2)
  
  beta_table$Beta_0[i] <- coef(fit)[1]
  beta_table$Beta_1[i] <- coef(fit)[2]
}

beta_table

```

**Interpretation:-**

The population regression line (black) is fixed:
ð‘Œ=2+3ð‘‹
Each sample regression line differs slightly due to random error.This shows sampling variability in least squares estimation.



### 2.Problem to demonstrate that Î²Ë†0 and Î²Ë† minimises RSS

**Step 1:** Generate xi from Uniform(5, 10) and mean centre the values. Generate
Îµi from N(0, 1). Calculate yi = 2 + 3xi + Îµi, i = 1,2,.., n. Take n=50 and seed=123.

**Step 2:** Now imagine that you only have the data on (xi, yi), i = 1, 2, .., n,
without knowing the mechanism that was used to generate the data in step 1.
Assuming a linear regression of the type yi = Î²0 + Î²xi + Îµi,and based on these data (xi, yi), i = 1, 2,.., n, obtain the least squares estimates of Î²0 and Î².

**Step 3:** Take a large number of grid values of (Î²0, Î²) that also include the least
squares estimates obtained from step 2. Compute the RSS for each parametric
choice of (Î²0, Î²), where RSS = (y1 âˆ’ Î²0 âˆ’ Î²x1)2 + (y2 âˆ’ Î²0 âˆ’ Î²x2)2 + ....(yn âˆ’Î²0 âˆ’ Î²xn)2. Find out for which combination of (Î²0, Î²), RSS is minimum.


```{r}
#step 1.
set.seed(123)
n=50
x=runif(n,5,10)
x=x-mean(x) 
e=rnorm(n,0,1)
y=2+3*x+e
y
```

```{r}
#Step 2:-
model1=lm(y~x)
coef(model1)
```

```{r}
#Step 3:-
beta0_grid=seq(1,3,length = 100)
beta1_grid=seq(2,4,length = 100)

RSS=matrix(NA, 100, 100)

for (i in 1:100) {
  for (j in 1:100) {
    RSS[i, j]=sum((y-beta0_grid[i]-beta1_grid[j]*x)^2)
  }
}

min_RSS=which(RSS == min(RSS), arr.ind = TRUE)

beta0_grid[min_RSS[1]]
beta1_grid[min_RSS[2]]

```
**Interpretation:-**
The minimum RSS occurs at the least squares estimates.Confirms that Ordinary least squares(OLS) estimators minimize RSS.


### 3.Problem to demonstrate that least square estimators are unbiased

**Step 1:**Generate xi(i = 1, 2, .., n) from Uniform(0, 1), Îµi(i = 1, 2, .., n) from
N(0, 1) and hence generate y using yi = Î²0 + Î²xi + Îµi.(Take Î²0 = 2, Î² = 3).

**Step 2:** On the basis of the data (xi,yi)(i = 1, 2, .., n) generated in Step 1,btain the least square estimates of Î²0 and Î².Repeat Steps 1-2, R = 1000 times. In each simulation obtain Î²Ë†0 and Î²Ë†.Finally,the least-square estimates will be given by the average of these estimated values.Compare these with the true Î²0 and Î² and comment.

**Take n = 50 and seed=123.**


```{r}
#step 1:-
set.seed(123)
x_3=runif(50,0,1)
x_3
e_3=rnorm(50,0,1)
e_3
y_3=2+3*x_3+e_3
y_3
model=lm(y_3~x_3)
model
summary(model)
```


```{r}
#Step 2:-
set.seed(123)
n=50
R=1000

beta0_hat=numeric(R)
beta1_hat=numeric(R)

for (i in 1:R) {
  x=runif(n,0,1)
  e=rnorm(n,0,1)
  y=2+3*x+e
  
  model2=lm(y~x)
  beta0_hat[i]=coef(model2)
  beta1_hat[i]=coef(model2)
}

mean(beta0_hat)
mean(beta1_hat)

```
**Interpretation:-**

Average of estimates are close to the true value.Hence, OLS estimators are unbiased.The simulation results show that the average of the estimated intercept (Î²Ì‚â‚€) and slope (Î²Ì‚â‚) over repeated samples is very close to the true parameter values Î²â‚€ = 2 and Î²â‚ = 3. This confirms that the least squares estimators are unbiased. Any deviation in individual samples is due to random error. As the number of simulations increases, the estimates converge to the true parameters.


### 4.Comparing several simple linear regressions

**Attach â€œBostonâ€ data from MASS library in R. Select median value of owner-occupied homes, as the response and per capita crime rate, nitrogen oxides concentration, proportion of blacks and percentage of lower status of the population as predictors.**

(a) Selecting the predictors one by one, run four separate linear regressions to
the data. Present the output in a single table.
(b) Which model gives the best fit?
(c) Compare the coefficients of the predictors from each model and comment on
the usefulness of the predictors.

```{r}
library(MASS)
data(Boston)
fit1=lm(medv~crim,data=Boston)
fit2=lm(medv~nox,data=Boston)
fit3=lm(medv~black, data=Boston)
fit4=lm(medv~lstat,data=Boston)

summary_table=data.frame(
  Predictor=c("crim", "nox", "black", "lstat"),
  Coefficient=c(coef(fit1), coef(fit2),coef(fit3),coef(fit4)),
  R_squared=c(summary(fit1)$r.squared,
                summary(fit2)$r.squared,
                summary(fit3)$r.squared,
                summary(fit4)$r.squared)
)

summary_table

```

**(b) Best fit:-**
The model with lstat has the highest RÂ².Best predictor of median house value.

**(c) Interpretation:-**

**crim:** Negative impact, weak explanatory power
**nox:** Strong negative relationship
**black:** Positive but weak predictor
**lstat:** Strongest negative effect and best fit

**Conclusion:-**
Among the four predictors considered, the percentage of lower status population (lstat) provides the best fit, as it has the highest RÂ² value. Crime rate (crim) and nitrogen oxides concentration (nox) show negative relationships with median house value, but with weaker explanatory power. The proportion of blacks (black) has a relatively weak effect. Overall, lstat is the most useful predictor of median house prices.
